{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d83e0e",
   "metadata": {},
   "source": [
    "# DATA CLEANING PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bb28c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Importing the neccessary python Libraries\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\"\"\"Creating the rainfall data path\"\"\"\n",
    "csv_path = 'C:\\\\Users\\\\DELL\\\\OneDrive\\\\Documents\\\\TASK 2\\\\Rainfall data 2002-2024.csv'\n",
    "\n",
    "\"\"\"Reading the rainfall data in CSV format (Visual crossing)\"\"\"\n",
    "csv_df = pd.read_csv(csv_path)\n",
    "\n",
    "\"\"\"Checking the number of Rows and Columns in the CSV file\"\"\"\n",
    "csv_df.shape\n",
    "\n",
    "\"\"\"Checking for the number of duplicates values in the CSV file across all columns\"\"\"\n",
    "csv_df.duplicated().sum()\n",
    "\n",
    "\"\"\"Deleting the observed duplicate values\"\"\"\n",
    "csv_df = csv_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "\"\"\"Converting the datetime column to datetime datatype from object datatype\"\"\"\n",
    "csv_df['datetime'] = pd.to_datetime(csv_df['datetime'])\n",
    "\n",
    "\"\"\"Fill all Nan row using the ffill method\"\"\"\n",
    "csv_df = csv_df.fillna(method='ffill')\n",
    "\n",
    "\"\"\"Creating dayoftheweek, month and season columns from the datetime column\"\"\"\n",
    "csv_df['dayofweek'] = csv_df['datetime'].dt.day_name()  \n",
    "csv_df['month'] = csv_df['datetime'].dt.month  \n",
    "csv_df['season'] = (csv_df['datetime'].dt.month % 12 + 3) // 3\n",
    "\n",
    "\"\"\"Deleting columns not needed for building the model\"\"\"\n",
    "csv_df = csv_df.drop(['feelslikemax', 'precipprob', 'feelslikemin', 'snow', 'snowdepth', 'datetime', 'name', \n",
    "                      'severerisk', 'sunrise', 'sunset', 'description', 'icon', 'stations', 'feelslike', 'preciptype',\n",
    "                      'conditions', 'windspeedmax', 'windspeedmin'], axis=1).reset_index(drop=True)\n",
    "\n",
    "\"\"\"Encoding the dayofweek categorical data to numeric data\"\"\"\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "csv_df['dayofweek'] = le.fit_transform(csv_df['dayofweek'])\n",
    "\n",
    "\"\"\"Creating ohe_df dataframe from csv_df\"\"\"\n",
    "ohe_df = csv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223a5de2",
   "metadata": {},
   "source": [
    "# DAILY FLOOD PREDICTION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a71254d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.75      0.86      1677\n",
      "         1.0       0.02      0.60      0.04        15\n",
      "\n",
      "    accuracy                           0.75      1692\n",
      "   macro avg       0.51      0.68      0.45      1692\n",
      "weighted avg       0.99      0.75      0.85      1692\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "\n",
    "# Split the data into features and target\n",
    "X = ohe_df.drop('flood', axis=1)\n",
    "y = ohe_df['flood']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform SMOTE to handle class imbalance\n",
    "sm = BorderlineSMOTE(k_neighbors=5, random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_res_scaled = scaler.fit_transform(X_train_res)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize Logistic Regression model with a different solver and increased max_iter\n",
    "lr = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
    "lr.fit(X_train_res_scaled, y_train_res)\n",
    "\n",
    "# Make predictions\n",
    "predictions = lr.predict(X_test_scaled)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb42831",
   "metadata": {},
   "source": [
    "# TESTING OF THE MODEL BASED OF VISUAL CROSSING 15 DAYS RANGE WEATHER API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85092dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on API data:\n",
      "[0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example API request to retrieve data\n",
    "try: \n",
    "    ResultBytes = urllib.request.urlopen(\"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/lagos%20nigeria?unitGroup=us&include=days&key=8BBTUCBUPQQ7HKQSCENEWPND5&contentType=json\")\n",
    "  \n",
    "    # Parse the results as JSON\n",
    "    jsonData = json.load(ResultBytes)\n",
    "    \n",
    "    # Extract relevant data from JSON response\n",
    "    data_points = jsonData['days']\n",
    "    \n",
    "    # Convert JSON data to DataFrame\n",
    "    api_data = pd.DataFrame(data_points)\n",
    "    \n",
    "    # Ensure datetime column is parsed correctly\n",
    "    api_data['datetime'] = pd.to_datetime(api_data['datetime'])\n",
    "    \n",
    "    # Extract day of the week, month, and season from datetime\n",
    "    api_data['dayofweek'] = api_data['datetime'].dt.day_name()  # Day of the week (Monday, Tuesday, etc.)\n",
    "    api_data['month'] = api_data['datetime'].dt.month  # Month of the year (1-12)\n",
    "    api_data['season'] = (api_data['datetime'].dt.month % 12 + 3) // 3  # Determine season\n",
    "    \n",
    "    # Use LabelEncoder to transform 'dayofweek' to numeric\n",
    "    le = LabelEncoder()\n",
    "    api_data['dayofweek'] = le.fit_transform(api_data['dayofweek'])\n",
    "    \n",
    "    # Define features used in your model\n",
    "    features1 = ['tempmax', 'tempmin', 'temp', 'dew', 'humidity', 'precip', 'precipcover', 'windgust', 'windspeed', 'winddir', \n",
    "                'pressure', 'cloudcover', 'visibility', 'solarradiation', 'solarenergy', 'uvindex', 'moonphase', \n",
    "                'dayofweek', 'month', 'season']\n",
    "    \n",
    "    # Filter and preprocess API data to match your model's features\n",
    "    X_api = api_data[features1] # Select relevant features\n",
    "    \n",
    "    # Scale the API data using the same scaler instance as used on training data\n",
    "    scaler = StandardScaler()\n",
    "    X_api_scaled = scaler.fit_transform(X_api)  # Use 'scaler' from your training step\n",
    "    \n",
    "    # Use the trained logistic regression model to predict on API data\n",
    "    predictions = lr.predict(X_api_scaled)\n",
    "    \n",
    "    # Print or display predictions\n",
    "    print(\"Predictions on API data:\")\n",
    "    print(predictions)\n",
    "    \n",
    "except urllib.error.HTTPError as e:\n",
    "    ErrorInfo= e.read().decode() \n",
    "    print('Error code: ', e.code, ErrorInfo)\n",
    "except urllib.error.URLError as e:\n",
    "    ErrorInfo= e.read().decode() \n",
    "    print('Error code: ', e.code, ErrorInfo)\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32156230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
